{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "driving-teaching",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import os\n",
    "import skimage.color as color\n",
    "import PIL\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "previous-stewart",
   "metadata": {},
   "source": [
    "## Load List of Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "educated-cardiff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.imgCap import load_images_list\n",
    "from zipfile import ZipFile\n",
    "\n",
    "# Extract Data\n",
    "if not os.path.exists('./Flickr_Data.zip'):\n",
    "    raise Exception('Dataset not found. Please read instructions above this cell and download dataset.')\n",
    "\n",
    "if not os.path.exists('./Flickr_Data'):\n",
    "    print(\"Extracting data ...\")\n",
    "    ZipFile('./Flickr_Data.zip', 'r').extractall('./')\n",
    "\n",
    "#Files with names of corresponding images\n",
    "train_image_list_path = './Flickr_Data/Flickr8k_text/Flickr_8k.trainImages.txt'\n",
    "test_image_list_path = './Flickr_Data/Flickr8k_text/Flickr_8k.testImages.txt'\n",
    "\n",
    "train_image_list = load_images_list(train_image_list_path)\n",
    "test_image_list = load_images_list(test_image_list_path)\n",
    "\n",
    "print('Total train images:',len(train_image_list))\n",
    "print('Total test images:', len(test_image_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fuzzy-plate",
   "metadata": {},
   "source": [
    "## Method to Convert Image from RGB to LaB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "iraqi-mouse",
   "metadata": {},
   "outputs": [],
   "source": [
    "image = Image.open('./Flickr_Data/Flickr8k_Dataset/2903617548_d3e38d7f88.jpg', mode='r')\n",
    "f = plt.figure()\n",
    "f.add_subplot(1,2, 1)\n",
    "plt.imshow(image)\n",
    "f.add_subplot(1,2, 2)\n",
    "lab_image = color.rgb2lab(np.asarray(image))\n",
    "plt.imshow(lab_image[:,:,0],cmap=\"gray\")\n",
    "plt.show(block=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dietary-canon",
   "metadata": {},
   "source": [
    "## Encode Image for Resnet Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "white-printer",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resize the original images to 299*299\n",
    "# Then, convert them to LaB images\n",
    "# \"L\" is the input for the later model and \"aB\" is the ground truth for model output\n",
    "\n",
    "train_data = np.zeros([600, 299, 299, 3])\n",
    "test_data = np.zeros([100, 299, 299, 3])\n",
    "\n",
    "images_path = './Flickr_Data/Flickr8k_Dataset'\n",
    "i = 0\n",
    "for image_name in train_image_list:\n",
    "        path = images_path + \"/\" + image_name\n",
    "        image = Image.open(path, mode='r')\n",
    "        image = image.resize((299,299))\n",
    "        x = color.rgb2lab(np.asarray(image))\n",
    "        x = x/255\n",
    "        x = x.reshape((1,299,299,3))\n",
    "        train_data[i,:,:,:] = x\n",
    "        i += 1\n",
    "        if i>=600:\n",
    "            break\n",
    "\n",
    "i = 0\n",
    "for image_name in test_image_list:\n",
    "        path = images_path + \"/\" + image_name\n",
    "        image = Image.open(path, mode='r')\n",
    "        image = image.resize((299,299))\n",
    "        x = color.rgb2lab(np.asarray(image))\n",
    "        x = x/255\n",
    "        x = x.reshape((1,299,299,3))\n",
    "        test_data[i,:,:,:] = x\n",
    "        i += 1\n",
    "        if i>=100:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "black-mortality",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = plt.figure()\n",
    "f.add_subplot(1,2, 1)\n",
    "plt.imshow(train_data[1,:,:,0],cmap=\"gray\")\n",
    "f.add_subplot(1,2, 2)\n",
    "plt.imshow(color.lab2rgb(train_data[1]*255))\n",
    "plt.show(block=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hidden-demonstration",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "f = plt.figure()\n",
    "f.add_subplot(1,2, 1)\n",
    "plt.imshow(test_data[1,:,:,0],cmap=\"gray\")\n",
    "f.add_subplot(1,2, 2)\n",
    "plt.imshow(color.lab2rgb(test_data[1]*255))\n",
    "plt.show(block=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "successful-tolerance",
   "metadata": {},
   "source": [
    "## Resnet Embedding and Stacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "minimal-divide",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications import InceptionResNetV2\n",
    "from tensorflow.keras import Model\n",
    "pre_trained_model = InceptionResNetV2(weights='imagenet')\n",
    "feature_extractor = Model(inputs=pre_trained_model.input,outputs=pre_trained_model.layers[-2].output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "modified-certification",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_embeddings = np.zeros([600, 299, 299, 3],dtype='float32')\n",
    "train_embeddings[:,:,:,0] = train_embeddings[:,:,:,1] = train_embeddings[:,:,:,2] = train_data[:,:,:,0]\n",
    "\n",
    "test_embeddings = np.zeros([100, 299, 299, 3],dtype='float32')\n",
    "test_embeddings[:,:,:,0] = test_embeddings[:,:,:,1] = test_embeddings[:,:,:,2] = test_data[:,:,:,0]\n",
    "\n",
    "train_emb = feature_extractor.predict(train_embeddings)\n",
    "test_emb = feature_extractor.predict(test_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "broke-alert",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('./encoded_images'):\n",
    "    os.mkdir('./encoded_images')\n",
    "np.save('./encoded_images/train_emb.npy',train_emb)\n",
    "np.save('./encoded_images/test_emb.npy',test_emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "social-referral",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_emb = np.load('./encoded_images/train_emb.npy')\n",
    "test_emb = np.load('./encoded_images/test_emb.npy')\n",
    "train_emb = train_emb.reshape([600,1,1,1536])\n",
    "test_emb = test_emb.reshape([100,1,1,1536])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "postal-pregnancy",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_embeddings = np.zeros([600,28,28,1536],dtype='float32')\n",
    "for i in range(600):\n",
    "    for j in range(28):\n",
    "        train_embeddings[i,j,j,:] = train_emb[i,0,0,:]\n",
    "        \n",
    "train_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "integrated-exposure",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_embeddings = np.zeros([100,28,28,1536],dtype='float32')\n",
    "for i in range(100):\n",
    "    for j in range(28):\n",
    "        test_embeddings[i,j,j,:] = test_emb[i,0,0,:]\n",
    "        \n",
    "test_embeddings.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "presidential-journalism",
   "metadata": {},
   "source": [
    "## prepare Data for Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "balanced-moderator",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = np.zeros([600, 224, 224, 3])\n",
    "test_data = np.zeros([100, 224, 224, 3])\n",
    "\n",
    "images_path = './Flickr_Data/Flickr8k_Dataset'\n",
    "i = 0\n",
    "for image_name in train_image_list:\n",
    "        path = images_path + \"/\" + image_name\n",
    "        image = Image.open(path, mode='r')\n",
    "        image = image.resize((224,224))\n",
    "        x = color.rgb2lab(np.asarray(image))\n",
    "        x = x/255\n",
    "        x = x.reshape((1,224,224,3))\n",
    "        train_data[i,:,:,:] = x\n",
    "        i += 1\n",
    "        if i>=600:\n",
    "            break\n",
    "\n",
    "i = 0\n",
    "for image_name in test_image_list:\n",
    "        path = images_path + \"/\" + image_name\n",
    "        image = Image.open(path, mode='r')\n",
    "        image = image.resize((224,224))\n",
    "        x = color.rgb2lab(np.asarray(image))\n",
    "        x = x/255\n",
    "        x = x.reshape((1,224,224,3))\n",
    "        test_data[i,:,:,:] = x\n",
    "        i += 1\n",
    "        if i>=100:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aquatic-dakota",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('./encoded_images/train_data.npy',train_data)\n",
    "np.save('./encoded_images/test_data.npy',test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "billion-acquisition",
   "metadata": {},
   "source": [
    "## Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "norwegian-crown",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Conv2D, AveragePooling2D, MaxPooling2D, Concatenate,UpSampling2D\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.applications import InceptionResNetV2\n",
    "from tensorflow.keras.layers import Dense, Dropout, Input, LSTM, Embedding, Add, Bidirectional, Concatenate, RepeatVector, GRU\n",
    "\n",
    "\n",
    "start = Input(shape=(224,224,1))\n",
    "encoder = Conv2D(64, (3, 3), activation='relu', padding='same', strides=(2,2))(start)\n",
    "encoder = Conv2D(128, (3, 3), activation='relu', padding='same', strides=(1,1))(encoder)\n",
    "encoder = Conv2D(128, (3, 3), activation='relu', padding='same', strides=(2,2))(encoder)\n",
    "encoder = Conv2D(256, (3, 3), activation='relu', padding='same', strides=(1,1))(encoder)\n",
    "encoder = Conv2D(256, (3, 3), activation='relu', padding='same', strides=(2,2))(encoder)\n",
    "encoder = Conv2D(512, (3, 3), activation='relu', padding='same', strides=(1,1))(encoder)\n",
    "encoder = Conv2D(512, (3, 3), activation='relu', padding='same', strides=(1,1))(encoder)\n",
    "encoder = Conv2D(256, (3, 3), activation='relu', padding='same', strides=(1,1))(encoder)\n",
    "\n",
    "feature_extractor = Input(shape=(28,28,1536))\n",
    "fusion = Concatenate()([encoder,feature_extractor])\n",
    "fusion = Conv2D(256, (1, 1), activation='relu', padding='same', strides=1)(fusion)\n",
    "\n",
    "decoder = Conv2D(128, (3, 3), activation=\"relu\", padding=\"same\")(fusion)\n",
    "decoder = UpSampling2D((2, 2))(decoder)\n",
    "decoder = Conv2D(64, (3, 3), activation=\"relu\", padding=\"same\")(decoder)\n",
    "decoder = Conv2D(64, (3, 3), activation=\"relu\", padding=\"same\")(decoder)\n",
    "decoder = UpSampling2D((2, 2))(decoder)\n",
    "decoder = Conv2D(32, (3, 3), activation=\"relu\", padding=\"same\")(decoder)\n",
    "decoder = Conv2D(2, (3, 3), activation=\"tanh\", padding=\"same\")(decoder)\n",
    "decoder = UpSampling2D((2, 2))(decoder)\n",
    "\n",
    "deep_color = Model([start,feature_extractor],decoder)\n",
    "\n",
    "deep_color.compile(optimizer='Adam',loss='mse',metrics=['accuracy'])\n",
    "\n",
    "deep_color.summary()\n",
    "#Train the neural network\n",
    "#model.fit(x=X, y=Y, batch_size=1, epochs=3000)\n",
    "#print(model.evaluate(X, Y, batch_size=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "known-velvet",
   "metadata": {},
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "soviet-function",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = np.load('./encoded_images/train_data.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "significant-alexander",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_input = train_data[:,:,:,0].reshape([600,224,224,1])\n",
    "train_input.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pleased-baseball",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "driven-vertex",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_output = train_data[:,:,:,1:]\n",
    "train_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "arranged-circular",
   "metadata": {},
   "outputs": [],
   "source": [
    "deep_color.fit([train_input,train_embeddings],train_output, epochs=20,batch_size=60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fd19ac9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
